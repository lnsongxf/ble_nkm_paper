\section{Results under Real-Time SAC-Learning}
\label{sec:estimation_dgl}
Our results in the previous section highlight that a BLE estimated under Algorithm II provides a better fit than the REE model. As an alternative to this algorithm, and a final robustness check, we consider the estimation of the same model under real-time SAC-learning. As we noted in Section \ref{sec:iterative_ble}, an iteratively E-stable equilibrium that results under Algorithm II is also E-stable, and hence it is also learnable under SAC-learning. Therefore, an alternative and indirect approach to estimate the underlying BLE is to let the agents learn in real-time over the estimation sample. Since the model is conditionally linear for a given set of belief coefficients, one can use the standard Kalman filter recursions to iterate the model forward. Then the beliefs are updated using the Kalman filter output. This is the standard approach in estimating adaptive learning models; see e.g Slobodyan \& Wouters (2012) and Milani (2005) for more details. While real-time learning estimations typically involve a constant-gain algorithm, we use our decreasing-gain version here in order to let the beliefs converge to the underlying BLE. \\
\noindent






Figure \ref{nkpc-sac-figure} shows the learning parameters for the mean and persistence along with the Kalman filter output. The learning parameters are initialized as follows: We use the unconditional moment of 0 for the intercept coefficients $\boldsymbol alpha_t$, and the diffuse moment 0 for the estimated variance of each variable $R_t$\footnote{The initial choice of first-order autocorrelations does not matter since $\beta_{1,t}=-\frac{1}{2}$ regardless of what $\beta_{0,t} is$.}. We use a period of five years over 1961:I-1965:IV as the transient period for the belief coefficients, and use the same sample period of 1966:I-2016:IV as in Section \ref{sec:estimation_ble} to estimate the model. \\
Table \ref{nkpc_sac} and Figure \ref{nkpc-sac-figure} report our estimation results: the first thing we notice is that, the learning parameters for persistence indeed converge, and their equilibrium values are fairly close to what we found in \ref{sec:estimation_ble} with $\boldsymbol \beta^{*}=(0.86, 0.89)$. This difference is due to small differences in the parameter estimates under BLE and SAC-learning estimations. While the learning parameter for output gap mean always remains close to zero, the inflation parameter is off-the equilibrium during the entire period and has not yet converged to zero at the end of the estimation sample. \\
\noindent
Next turning to the parameter estimates, a noticeable difference compared with BLE estimation arises in the steady-state values of inflation and interest-rate, which turn out to be lower in this case. This difference is expected since the learning coefficients for the mean are well above zero on average in this case, which drives down the estimates of the steady-state parameters\footnote{Shutting off the learning dynamics about mean parameters indeed yields steady-state values similar to BLE.}. Another difference arises in the first-order autocorrelation of the output gap shock, which turns higher under SAC-learning with a mode of $0.57$, compared with $0.42$ under BLE. Other than these small differences, all parameter estimates are fairly close under SAC-learning and BLE. The likelihood turns out to be $-341$, which is still better than the fit of REE and almost identical to that of BLE. This result suggests that transitory dynamics or the time-variation in the learning parameters do not improve the model fit in this decreasing gain learning context. 
This result suggests the transitory dynamics or the time-variation in the learning parameters do not improve the model fit in a decreasing gain learning context. Overall, these results also allow us to illustrate the advantage of using Algorithm II to estimate a BLE: The estimation under real-time learning requires a relatively large burn-in sample for the convergence of first-order autocorrelation coefficients, which might become an issue since macroeconomic time series are typically not very long. Furthermore, as we have already seen in the previous sections, the simulations under learning have a relatively large Monte-Carlo variance. In other words, while the simulations converge to the underlying BLE on average, there might be relatively large deviations from the underlying fixed-point for any given simulation. Hence what comes out of the estimation in a real-time learning setup may not always accurately reflect the underlying fixed-point. While these issues evidently do not arise in our estimation of the standard 3-equation model, they are more likely to be pronounced in the estimation of a medium-scale model such as Smets-Wouters (2007), in which case using the fixed-point approach becomes more advantageous. \\